{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "State notebook purpose here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get source folder and append to sys directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/runnable_program\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "print(os.path.abspath(PROJ_ROOT))\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)\n",
    "# Data path example\n",
    "#pump_data_path = os.path.join(PROJ_ROOT,\n",
    "#                              \"data\",\n",
    "#                              \"raw\",\n",
    "#                              \"pumps_train_values.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Import libraries and write settings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 1\n",
    "# Use %aimport module to reload each module\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis/Modeling\n",
    "Do work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "feed = \"http://dosairnowdata.org/dos/RSS/HoChiMinhCity/HoChiMinhCity-PM2.5.xml\"\n",
    "NewsFeed = feedparser.parse(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                0\n",
       "bozo                                                        False\n",
       "entries         [{'title': '2021-07-02 16:00:00', 'title_detai...\n",
       "feed            {'title': 'Ho Chi Minh City', 'title_detail': ...\n",
       "headers         {'content-type': 'application/xml', 'content-l...\n",
       "etag                                         \"1d7f-5c633f124a8a2\"\n",
       "updated                             Sat, 03 Jul 2021 08:34:34 GMT\n",
       "updated_parsed                 (2021, 7, 3, 8, 34, 34, 5, 184, 0)\n",
       "href            http://dosairnowdata.org/dos/RSS/HoChiMinhCity...\n",
       "status                                                        200\n",
       "encoding                                                    utf-8\n",
       "version                                                     rss20\n",
       "namespaces                                                     {}"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bozo</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>entries</th>\n      <td>[{'title': '2021-07-02 16:00:00', 'title_detai...</td>\n    </tr>\n    <tr>\n      <th>feed</th>\n      <td>{'title': 'Ho Chi Minh City', 'title_detail': ...</td>\n    </tr>\n    <tr>\n      <th>headers</th>\n      <td>{'content-type': 'application/xml', 'content-l...</td>\n    </tr>\n    <tr>\n      <th>etag</th>\n      <td>\"1d7f-5c633f124a8a2\"</td>\n    </tr>\n    <tr>\n      <th>updated</th>\n      <td>Sat, 03 Jul 2021 08:34:34 GMT</td>\n    </tr>\n    <tr>\n      <th>updated_parsed</th>\n      <td>(2021, 7, 3, 8, 34, 34, 5, 184, 0)</td>\n    </tr>\n    <tr>\n      <th>href</th>\n      <td>http://dosairnowdata.org/dos/RSS/HoChiMinhCity...</td>\n    </tr>\n    <tr>\n      <th>status</th>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>encoding</th>\n      <td>utf-8</td>\n    </tr>\n    <tr>\n      <th>version</th>\n      <td>rss20</td>\n    </tr>\n    <tr>\n      <th>namespaces</th>\n      <td>{}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature shape:  (12, 5, 33)\nLabel shape:  (12,)\nData array:\nshape of arr:  (12, 5, 33)\nshape of loaded_array:  (12, 5, 33)\nYes, both the arrays are same\nLabel array:\nshape of arr:  (12,)\nshape of loaded_array:  (12,)\nYes, both the arrays are same\nInput have been created\n"
     ]
    }
   ],
   "source": [
    "from features.calculate_AQI import categorize_AQI\n",
    "from features.extract_features import data_preprocessing\n",
    "from data.create_input_for_models import create_input_for_model\n",
    "train = pd.DataFrame.from_dict(NewsFeed, orient='index')\n",
    "train\n",
    "train2 = pd.DataFrame.from_dict(train.loc['entries', :].values[0])\n",
    "train2 = train2[['title', 'aqi']]\n",
    "train2.rename(columns={'title': 'time', 'aqi': 'AQI_h'}, inplace=True)\n",
    "train2 = train2.astype({'time': 'datetime64[ns]', 'AQI_h': 'float'})\n",
    "train2['site_id'] = 49\n",
    "train2.set_index(['site_id', 'time'], inplace=True)\n",
    "train2['AQI_h_label'] = categorize_AQI(train2['AQI_h'])\n",
    "train2['AQI_h_I'] = train2['AQI_h_label'].cat.codes + 1\n",
    "train2['Continous length'] = 0\n",
    "train, y_train = create_input_for_model(train2, timesteps=[5], target_hour=[1], output_path=os.path.join(PROJ_ROOT, \"data\", \"model_input\", \"aqinow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.to_csv(\"past_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/runnable_program'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "[[[ 1.0000000e+00  1.0000000e+00  0.0000000e+00 ...  5.2000000e+01\n",
      "    5.2000000e+01  5.2000000e+01]\n",
      "  [ 1.0000000e+00  1.0000000e+00  0.0000000e+00 ...  5.5000000e+01\n",
      "    5.5000000e+01  5.5000000e+01]\n",
      "  [-7.3913044e-01  0.0000000e+00  0.0000000e+00 ...  5.6500000e+01\n",
      "    5.6500000e+01  5.6500000e+01]\n",
      "  [-1.3043478e-01  0.0000000e+00  0.0000000e+00 ...  5.3400002e+01\n",
      "    5.3400002e+01  5.3400002e+01]\n",
      "  [-4.3478262e-02  0.0000000e+00  0.0000000e+00 ...  5.2500000e+01\n",
      "    5.2500000e+01  5.2500000e+01]]\n",
      "\n",
      " [[ 1.0000000e+00  1.0000000e+00  0.0000000e+00 ...  5.5000000e+01\n",
      "    5.5000000e+01  5.5000000e+01]\n",
      "  [-7.3913044e-01  0.0000000e+00  0.0000000e+00 ...  5.6500000e+01\n",
      "    5.6500000e+01  5.6500000e+01]\n",
      "  [-1.3043478e-01  0.0000000e+00  0.0000000e+00 ...  5.3400002e+01\n",
      "    5.3400002e+01  5.3400002e+01]\n",
      "  [-4.3478262e-02  0.0000000e+00  0.0000000e+00 ...  5.2500000e+01\n",
      "    5.2500000e+01  5.2500000e+01]\n",
      "  [-2.1739130e-01  0.0000000e+00  0.0000000e+00 ...  5.2000000e+01\n",
      "    5.2000000e+01  5.2000000e+01]]\n",
      "\n",
      " [[-7.3913044e-01  0.0000000e+00  0.0000000e+00 ...  5.6500000e+01\n",
      "    5.6500000e+01  5.6500000e+01]\n",
      "  [-1.3043478e-01  0.0000000e+00  0.0000000e+00 ...  5.3400002e+01\n",
      "    5.3400002e+01  5.3400002e+01]\n",
      "  [-4.3478262e-02  0.0000000e+00  0.0000000e+00 ...  5.2500000e+01\n",
      "    5.2500000e+01  5.2500000e+01]\n",
      "  [-2.1739130e-01  0.0000000e+00  0.0000000e+00 ...  5.2000000e+01\n",
      "    5.2000000e+01  5.2000000e+01]\n",
      "  [ 7.3913044e-01  1.0000000e+00  0.0000000e+00 ...  5.1375000e+01\n",
      "    5.1375000e+01  5.1375000e+01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-4.3478262e-02  0.0000000e+00  0.0000000e+00 ...  5.2454544e+01\n",
      "    5.2454544e+01  5.2454544e+01]\n",
      "  [-7.3913044e-01  0.0000000e+00  0.0000000e+00 ...  5.2181820e+01\n",
      "    5.2166668e+01  5.2166668e+01]\n",
      "  [-5.6521738e-01  0.0000000e+00  0.0000000e+00 ...  5.1181820e+01\n",
      "    5.1250000e+01  5.1307693e+01]\n",
      "  [-5.6521738e-01  0.0000000e+00  0.0000000e+00 ...  4.9545456e+01\n",
      "    5.0500000e+01  5.0714287e+01]\n",
      "  [-8.2608694e-01  0.0000000e+00  0.0000000e+00 ...  4.7909092e+01\n",
      "    4.9000000e+01  5.0200001e+01]]\n",
      "\n",
      " [[-7.3913044e-01  0.0000000e+00  0.0000000e+00 ...  5.2181820e+01\n",
      "    5.2166668e+01  5.2166668e+01]\n",
      "  [-5.6521738e-01  0.0000000e+00  0.0000000e+00 ...  5.1181820e+01\n",
      "    5.1250000e+01  5.1307693e+01]\n",
      "  [-5.6521738e-01  0.0000000e+00  0.0000000e+00 ...  4.9545456e+01\n",
      "    5.0500000e+01  5.0714287e+01]\n",
      "  [-8.2608694e-01  0.0000000e+00  0.0000000e+00 ...  4.7909092e+01\n",
      "    4.9000000e+01  5.0200001e+01]\n",
      "  [-3.9130434e-01  0.0000000e+00  0.0000000e+00 ...  4.7818180e+01\n",
      "    4.7250000e+01  4.9562500e+01]]\n",
      "\n",
      " [[-5.6521738e-01  0.0000000e+00  0.0000000e+00 ...  5.1181820e+01\n",
      "    5.1250000e+01  5.1307693e+01]\n",
      "  [-5.6521738e-01  0.0000000e+00  0.0000000e+00 ...  4.9545456e+01\n",
      "    5.0500000e+01  5.0714287e+01]\n",
      "  [-8.2608694e-01  0.0000000e+00  0.0000000e+00 ...  4.7909092e+01\n",
      "    4.9000000e+01  5.0200001e+01]\n",
      "  [-3.9130434e-01  0.0000000e+00  0.0000000e+00 ...  4.7818180e+01\n",
      "    4.7250000e+01  4.9562500e+01]\n",
      "  [ 4.3478262e-02  0.0000000e+00  0.0000000e+00 ...  4.7545456e+01\n",
      "    4.7583332e+01  4.9294117e+01]]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics, initializers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from data.create_load_transform_processed_data import create_tensorflow_dataset\n",
    "\n",
    "def get_model_name(timesteps, target_hour):\n",
    "    return 'model_{}_{}.h5'.format(timesteps, target_hour)\n",
    "\n",
    "def get_model_weigts_name(timesteps, target_hour):\n",
    "    return 'weights_{}_{}.ckpt'.format(timesteps, target_hour)\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))\n",
    "def mean_absolute_percentage_error(y_true, y_pred,\n",
    "                                   sample_weight=None,\n",
    "                                   multioutput='uniform_average'):\n",
    "    epsilon = np.finfo(np.float64).eps\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
    "    output_errors = np.average(mape,\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "os.path.abspath(PROJ_ROOT)\n",
    "model_path = os.path.join(PROJ_ROOT, 'models', 'combined', 'model_5_1')\n",
    "model = keras.models.load_model(model_path, custom_objects={'LeakyReLU': layers.LeakyReLU(alpha=0.01),\n",
    "                                               'root_mean_squared_error': root_mean_squared_error})\n",
    "train_tf, steps_train = create_tensorflow_dataset(train, y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((1, 5, 33), (1,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.23076923, -0.30769231,  0.69230769, -0.15384615, -0.38461538,\n",
       "       -0.30769231, -0.46153846, -1.        , -0.53846154, -0.76923077,\n",
       "       -0.23076923, -0.46153846])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics, initializers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_model_name(timesteps, target_hour):\n",
    "    return 'model_{}_{}.h5'.format(timesteps, target_hour)\n",
    "\n",
    "def get_model_weigts_name(timesteps, target_hour):\n",
    "    return 'weights_{}_{}.ckpt'.format(timesteps, target_hour)\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))\n",
    "def mean_absolute_percentage_error(y_true, y_pred,\n",
    "                                   sample_weight=None,\n",
    "                                   multioutput='uniform_average'):\n",
    "    epsilon = np.finfo(np.float64).eps\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
    "    output_errors = np.average(mape,\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "\n",
    "for hour in [1]:\n",
    "    rmse = []\n",
    "    r2 = []\n",
    "    mae = []\n",
    "    mape = []\n",
    "    for timestep in range(1, 13):    \n",
    "        batch_size = 700\n",
    "        rand = random.randint(0, len(y_test)-40)\n",
    "        if len(y_test) % batch_size != 0:\n",
    "            remain_count = len(y_test)%batch_size\n",
    "            test = test[remain_count:]\n",
    "            y_test = y_test[remain_count:]\n",
    "        test_data_tf, test_steps_per_epochs = create_tensorflow_dataset(test, y_test, batch_size)\n",
    "        \n",
    "#         model = create_model(batch_size=batch_size, timestep=timestep,features=7, dropout = 0.2)\n",
    "        model = keras.models.load_model(root_path + 'program/saved_models/latest_model/Hanoi/{}/model_of_{}_hour/{}'.format(\n",
    "            timestep, hour, get_model_name(timestep, hour)),\n",
    "                                custom_objects={'LeakyReLU': layers.LeakyReLU(alpha=0.01),\n",
    "                                               'root_mean_squared_error': root_mean_squared_error})\n",
    "        model.load_weights('{}{}/model_of_{}_hour/{}'.format(_model_path, timestep, hour,\n",
    "                                         get_model_weigts_name(timesteps=timestep, target_hour=hour))).expect_partial()\n",
    "        predict = model.predict(test_data_tf, steps=test_steps_per_epochs).ravel()\n",
    "        scaler_AQI = MinMaxScaler(feature_range=(-1,1))\n",
    "        scaler_AQI.fit(thudohanoi_df['AQI_h'].values.reshape(-1, 1))\n",
    "        y_test = scaler_AQI.inverse_transform(y_test.reshape(-1, 1))\n",
    "        predict = scaler_AQI.inverse_transform(predict.reshape(-1, 1))\n",
    "        print(\"=============================================\\n\")\n",
    "#         print(\"Predict\")\n",
    "#         predict_vs_truth = pd.DataFrame({'predict': predict[rand:rand+20],\n",
    "#                                         'truth': y_test[rand:rand+20]})\n",
    "#         print(predict_vs_truth)\n",
    "        print(\"R2: {}\".format(r2_score(predict, y_test)))\n",
    "        print(\"Root mean squared error: {}\".format(mean_squared_error(predict, y_test, squared=False)))\n",
    "        print(\"Mean absolute percentage error: {}\".format(mean_absolute_percentage_error(predict, y_test)))\n",
    "        print(\"Mean absolute error: {}\".format(mean_absolute_error(predict, y_test)))\n",
    "        \n",
    "        rmse.append(mean_squared_error(predict, y_test, squared=False))\n",
    "        r2.append(r2_score(predict, y_test))\n",
    "        mape.append(mean_absolute_percentage_error(predict, y_test))\n",
    "        mae.append(mean_absolute_error(predict, y_test))\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_subplot()\n",
    "    ax.plot(range(1,13),rmse)\n",
    "    ax.plot(range(1,13),r2)\n",
    "    ax.plot(range(1,13),mape)\n",
    "    ax.plot(range(1,13),mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Show graphs and stats here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next Steps\n",
    "Summarize findings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('aqi': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "interpreter": {
   "hash": "42a6f2126429ef925a6fad1ea37228e960d97f17ff7e5115d7c9932b5f26640c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}