{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import glob\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/30.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/9.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/32.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/11.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/40.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/28.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/49.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/46.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/10.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/8.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/25.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/7.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/16.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/42.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/44.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/37.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/1.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/13.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/31.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/26.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/12.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/15.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/39.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/14.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/47.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/48.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/33.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/27.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/41.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/24.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/35.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/43.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/45.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/34.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/29.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/38.csv\n",
      "Currently processing file \n",
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data/36.csv\n"
     ]
    }
   ],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "root_path = r'/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/'\n",
    "_thudohanoi_data_path = r'/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/Data/thudohanoi/refined_data'\n",
    "_thudohanoi_files = glob.glob(_thudohanoi_data_path + '/*.csv')\n",
    "\n",
    "thudohanoi_df = pd.DataFrame()\n",
    "for file in _thudohanoi_files:\n",
    "    print('Currently processing file \\n{}'.format(file))\n",
    "    thudohanoi_df = thudohanoi_df.append(pd.read_csv(file, parse_dates=True, index_col=['site_id', 'time'],\n",
    "                                                    dtype={'CO': np.float64, 'NO2': np.float64,'PM25': np.float64,\n",
    "                                                          'AQI_h': np.float64, 'AQI_h_I': np.int, 'site_id': np.int}))\n",
    "    \n",
    "# Remove site 16 because of some inconsistency in data\n",
    "thudohanoi_df = thudohanoi_df[(thudohanoi_df.index.get_level_values(0) == 49)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "def data_preprocessing(df, hour = 1, timesteps = 12, debug = False):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        + raw df contains all data \n",
    "            columns: ['PM25', 'AQI_h', 'AQI_h_I']\n",
    "            index: ['time']\n",
    "        + the predict hour to make data\n",
    "    Ouput: \n",
    "        + Processed train data:\n",
    "        + Target for the train data:\n",
    "        + Classes of target for spliting the data\n",
    "    \"\"\"\n",
    "\n",
    "    def chunker(seq, size):\n",
    "        return (seq.iloc[pos:pos + size] for pos in range(0, len(seq)-size))\n",
    "\n",
    "    def chunker_special(seq, target_hour, size, feature_cols, target_cols = ['AQI_h', 'AQI_h_I'], debug = False):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            + Dataframe with PM25, AQI_h, AQI_h_I, Continous length and some other columns\n",
    "                - If data doesn't have Continous length columns, add 1 to data\n",
    "                - Take only PM25, date columns as train\n",
    "                - Take only AQI_h, AQI_h_I as target\n",
    "                - Data must only have time as index\n",
    "            + size: length of each chunk\n",
    "            + target_hour: labels for the hour which will be predicted\n",
    "        Ouput: 2 lists: chunk_data, target\n",
    "            + chunk_data: original data chunked to a specific timestep, have shape [..., timeframe, features]\n",
    "            + target: Label for each chunked train/test data, have shape [..., 1]\n",
    "        \"\"\"\n",
    "\n",
    "        timerange = pd.Timedelta(hours=size + target_hour)\n",
    "        pos = 0\n",
    "        length = seq.shape[0]\n",
    "        continous_length_index = seq.columns.get_loc('Continous length')\n",
    "        if debug == True:\n",
    "            set_trace()\n",
    "        while pos < (length - size):\n",
    "            try:\n",
    "                if seq.iloc[pos + size + target_hour].name - seq.iloc[pos].name == timerange:\n",
    "                    while seq.iloc[pos + size + target_hour].name - seq.iloc[pos].name == timerange:\n",
    "                        chunk_tmp = seq.iloc[pos:pos + size].loc[:,feature_cols]\n",
    "                        target_tmp = seq.iloc[pos + size + target_hour].loc['AQI_h']\n",
    "                        multiclass_tmp = seq.iloc[pos + size + target_hour].loc['AQI_h_I']\n",
    "                        yield chunk_tmp, target_tmp, multiclass_tmp\n",
    "                        pos += 1\n",
    "                        if pos + size + target_hour >= length - size:\n",
    "                            return None\n",
    "                else:\n",
    "                    tmp_pos = pos + size + target_hour\n",
    "                    while seq.iloc[tmp_pos + size + target_hour].name - seq.iloc[tmp_pos].name != timerange:\n",
    "                        # set_trace()\n",
    "                        tmp_pos += int(seq.iloc[tmp_pos, continous_length_index])\n",
    "                    pos = tmp_pos\n",
    "            except IndexError:\n",
    "                set_trace()\n",
    "                print(\"Current position is: {}\".format(pos))\n",
    "                print(\"Current tmp position is: {}\".format(tmp_pos))\n",
    "                pos = length - size\n",
    "        return None\n",
    "# ===========================================================================================================================\n",
    "    hour = hour - 1\n",
    "    df_copy = df.copy()\n",
    "    exclude_cols = ['Continous length', 'AQI_h_I', 'PM25']\n",
    "    feature_cols = df_copy.drop(exclude_cols, axis=1).columns\n",
    "    # Remember: data shape is (datapoint, 12, 7)\n",
    "    # Remember: target shape is (datapoint, 1, 1)\n",
    "    site_ids = list(df_copy.index.get_level_values(0).unique())\n",
    "    train = []\n",
    "    y = []\n",
    "    multiclass_y = []\n",
    "    for site in site_ids:\n",
    "        if (site == 48) or (site == 49):\n",
    "            tmp = df_copy.loc[site]\n",
    "            tmp = tmp.iloc[2:]\n",
    "            generator = chunker_special(seq=tmp, target_hour=hour, size=timesteps, feature_cols=feature_cols)\n",
    "            for train_tmp, y_tmp, multiclass_tmp in generator:\n",
    "                train.append(list(train_tmp.values))\n",
    "                y.append(y_tmp)\n",
    "                multiclass_y.append(multiclass_tmp)\n",
    "        else:\n",
    "            # Data\n",
    "            tmp_train = df_copy.loc[site, feature_cols].copy()\n",
    "            tmp_train = tmp_train.iloc[2:]\n",
    "            generator = chunker(tmp_train, timesteps)\n",
    "            for i in generator:\n",
    "                train.append(list(i.values))\n",
    "            if hour != 0:\n",
    "                train = train[:-hour]\n",
    "            # Target\n",
    "            tmp_target = df_copy.loc[site, ['AQI_h', 'AQI_h_I']].copy()\n",
    "            tmp_target = tmp_target.iloc[2:]\n",
    "            tmp_target = tmp_target.shift(-timesteps - hour).dropna()\n",
    "            tmp_target_y = tmp_target[['AQI_h']].values.ravel()\n",
    "            tmp_target_multiclass_y = tmp_target[['AQI_h_I']].values.ravel()\n",
    "            y = y + list(tmp_target_y)\n",
    "            multiclass_y = multiclass_y + list(tmp_target_multiclass_y)\n",
    "    train = np.array(train)\n",
    "    y = np.array(y)\n",
    "    multiclass_y = np.array(multiclass_y)\n",
    "    print(\"Feature shape: \",train.shape)\n",
    "    print(\"Label shape: \",y.shape)\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    return train, y, multiclass_y\n",
    "\n",
    "def extract_time_features(df):\n",
    "    # Job: Expand time in data\n",
    "    time_index = df.index.get_level_values(1)\n",
    "    df_time_features = pd.DataFrame()\n",
    "    df_time_features['Hour'] = time_index.hour.astype(float)\n",
    "    df_time_features['Month'] = time_index.month.astype(float)\n",
    "    df_time_features['Day of Week'] = time_index.dayofweek.astype(float)\n",
    "    df_time_features['Day of Month'] = time_index.day.astype(float)\n",
    "    df_time_features['Days in Month'] = time_index.daysinmonth.astype(float)\n",
    "    df_time_features['Year'] = time_index.year.astype(float)\n",
    "\n",
    "    # Job: Encode time cyclical data\n",
    "    hour_in_day = 23\n",
    "    df_time_features['sin_hour'] = np.sin(2*np.pi*df_time_features['Hour']/hour_in_day)\n",
    "    df_time_features['cos_hour'] = np.cos(2*np.pi*df_time_features['Hour']/hour_in_day)\n",
    "    month_in_year = 12\n",
    "    df_time_features['sin_month'] = np.sin(2*np.pi*df_time_features['Month']/month_in_year)\n",
    "    df_time_features['cos_month'] = np.cos(2*np.pi*df_time_features['Month']/month_in_year)\n",
    "    day_in_week = 6\n",
    "    df_time_features['sin_dayweek'] = np.sin(2*np.pi*df_time_features['Day of Week']/day_in_week)\n",
    "    df_time_features['cos_dayweek'] = np.cos(2*np.pi*df_time_features['Day of Week']/day_in_week)\n",
    "    df_time_features['sin_daymonth'] = np.sin(2*np.pi*df_time_features['Day of Month']/df_time_features['Days in Month'])\n",
    "    df_time_features['cos_daymonth'] = np.cos(2*np.pi*df_time_features['Day of Month']/df_time_features['Days in Month'])\n",
    "    # One hot encode year data\n",
    "    one_hot_df = pd.get_dummies(df_time_features['Year'], drop_first=True, prefix='year')\n",
    "    df_time_features = df_time_features.join(one_hot_df)\n",
    "    # Input weekday/weekend/holiday data\n",
    "    vn_holidays = np.array(list(holidays.VN(years=[2015,2016,2017,2018,2019,2020,2021]).keys()))\n",
    "    holiday_mask = np.isin(time_index.date, vn_holidays)\n",
    "    masks = (holiday_mask) | (df_time_features['Day of Week'].values == 5) | (df_time_features['Day of Week'].values == 6)\n",
    "    df_time_features['day_off'] = np.where(masks == True, 1, 0)\n",
    "    df_time_features = df_time_features.drop(columns=['Day of Month', 'Month', 'Day of Week', 'Days in Month', 'Year', 'Hour'])\n",
    "#     Input lagged data\n",
    "    windows = list(range(1,13))\n",
    "    windows.append(24)\n",
    "    for window in windows:\n",
    "        feature = 'AQI_h'\n",
    "        series_rolled = df['AQI_h'].rolling(window=window, min_periods=0)\n",
    "        series_mean = series_rolled.mean().shift(1).reset_index()\n",
    "        series_std = series_rolled.std().shift(1).reset_index()\n",
    "        df_time_features[f\"{feature}_mean_lag{window}\"] = series_mean['AQI_h'].values\n",
    "#         df_time_features[f\"{feature}_std_lag{window}\"] = series_std['AQI_h'].values\n",
    "        df_time_features.fillna(df_time_features.mean(), inplace=True)\n",
    "        df_time_features.fillna(df['AQI_h'].mean(), inplace=True)\n",
    "\n",
    "    return df_time_features.values, df_time_features.columns\n",
    "\n",
    "def add_features(df):\n",
    "\n",
    "    # Change all data to numpy, then concatenate those numpy.\n",
    "    # Then construct the dataframe to old frame. This can work\n",
    "    data_df = df[['PM25', 'AQI_h', 'AQI_h_I', 'Continous length']].copy()\n",
    "\n",
    "    # Job: Normalize train data\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    for col in ['PM25', 'AQI_h']:\n",
    "        data_df[[col]] = scaler.fit_transform(data_df[[col]])\n",
    "\n",
    "    columns = ['site_id', 'time', 'PM25', 'AQI_h', 'AQI_h_I', 'Continous length']\n",
    "    df_numpy = data_df.reset_index().to_numpy()\n",
    "\n",
    "    # Add onehot site label\n",
    "    one_hot_site = pd.get_dummies(data_df.index.get_level_values(0), prefix='site', drop_first=True).astype(int)\n",
    "    columns.extend(one_hot_site.columns)\n",
    "    # Add onehot air category\n",
    "    one_hot_cat = pd.get_dummies(data_df['AQI_h_I'], drop_first=True, prefix='cat').astype(int)\n",
    "    columns.extend(one_hot_cat.columns)\n",
    "    # Add time features\n",
    "    time_features, time_columns = extract_time_features(data_df)\n",
    "    columns.extend(time_columns)\n",
    "    df_numpy = np.concatenate([df_numpy, one_hot_site.values, one_hot_cat.values, time_features], axis=1)\n",
    "    \n",
    "\n",
    "    final_df = pd.DataFrame(df_numpy, columns=columns).set_index(['site_id', 'time'])\n",
    "    for float_col in final_df.loc[:, final_df.dtypes == float].columns:\n",
    "        final_df.loc[:, float_col] = final_df.loc[:, float_col].values.round(6)\n",
    "    return final_df\n",
    "\n",
    "def generate_train_test_set_by_time(df, ratio = 0.1):\n",
    "    # Generate test set by taking the lastest 10% data from each site\n",
    "#     train_df = df.drop(index=(48), level=0).copy()\n",
    "    train_df = df.copy()\n",
    "    latest_time = train_df.index.get_level_values(1).max()\n",
    "    oldest_time = train_df.index.get_level_values(1).min()\n",
    "    cutoff_hour = (latest_time - oldest_time).total_seconds()\n",
    "    cutoff_hour = cutoff_hour // 3600\n",
    "    cutoff_hour = cutoff_hour * ratio\n",
    "    test_df = train_df[train_df.index.get_level_values(1) >= (latest_time - pd.Timedelta(hours=cutoff_hour))]\n",
    "    train_df = train_df[train_df.index.get_level_values(1) < (latest_time - pd.Timedelta(hours=cutoff_hour))]\n",
    "    # Generate train_test set for site 48 and 49\n",
    "#     train_df_48 = df[df.index.get_level_values(0) == 48]\n",
    "#     latest_time = train_df_48.index.get_level_values(1).max()\n",
    "#     oldest_time = train_df_48.index.get_level_values(1).min()\n",
    "#     cutoff_hour = (latest_time - oldest_time).total_seconds()\n",
    "#     cutoff_hour = cutoff_hour // 3600\n",
    "#     cutoff_hour = cutoff_hour * ratio\n",
    "#     test_df = test_df.append(train_df_48[train_df_48.index.get_level_values(1) >= (latest_time - pd.Timedelta(hours=cutoff_hour))])\n",
    "#     train_df = train_df.append(train_df_48[train_df_48.index.get_level_values(1) < (latest_time - pd.Timedelta(hours=cutoff_hour))])\n",
    "    return train_df, test_df\n",
    "def generate_train_test_set_by_skfold(df):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    # Generate test set by taking 10% data from everysite by stratified kfold method\n",
    "    df_copy = df.copy()\n",
    "    site_ids = list(df_copy.index.get_level_values(0).unique())\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    for site in site_ids:\n",
    "        site_df = df_copy.loc[site].copy()\n",
    "        y_multiclass = site_df['AQI_h_I'].values\n",
    "        for train_index, test_index in skf.split(np.zeros(len(y_multiclass)), y_multiclass):\n",
    "            site_train, site_test = site_df.iloc[train_index], site_df.iloc[test_index]\n",
    "            train_df = train_df.append(site_train)\n",
    "            test_df = test_df.append(site_test)\n",
    "            break\n",
    "\n",
    "    return train_df, test_df\n",
    "def reshape_array_and_save_to_path(arr_data, arr_label, path, timesteps, target_hour, data_type=\"Train\"):\n",
    "    # reshaping the array from 3D \n",
    "    # matrice to 2D matrice. \n",
    "    arr_data_reshaped = arr_data.reshape(arr_data.shape[0], -1)\n",
    "    arr_label_reshaped = arr_label.reshape(arr_label.shape[0], -1)\n",
    "    \n",
    "    # saving reshaped array to file.\n",
    "    saved_data = np.savez_compressed(path + \"/{}_{}_{}_data.npz\".format(timesteps, target_hour, data_type), arr_data_reshaped)\n",
    "    saved_label = np.savez_compressed(path + \"/{}_{}_{}_label.npz\".format(timesteps, target_hour, data_type), arr_label_reshaped)\n",
    "    \n",
    "    # retrieving data from file.\n",
    "    loaded_arr_data_file = np.load(path + \"/{}_{}_{}_data.npz\".format(timesteps, target_hour, data_type), allow_pickle=True)\n",
    "    loaded_arr_label_file = np.load(path + \"/{}_{}_{}_label.npz\".format(timesteps, target_hour, data_type), allow_pickle=True)\n",
    "    loaded_arr_data = loaded_arr_data_file['arr_0']\n",
    "    loaded_arr_data_file.close()\n",
    "    loaded_arr_label = loaded_arr_label_file['arr_0'].ravel()\n",
    "    loaded_arr_label_file.close()\n",
    "    # This loadedArr is a 2D array, therefore\n",
    "    # we need to convert it to the original \n",
    "    # array shape.reshaping to get original \n",
    "    # matrice with original shape. \n",
    "    loaded_arr_data = loaded_arr_data.reshape( \n",
    "        loaded_arr_data.shape[0], loaded_arr_data.shape[1] // arr_data.shape[2], arr_data.shape[2])\n",
    "    \n",
    "    features_save = np.save(path+\"/features.npy\", arr_data.shape[2])\n",
    "    # check the shapes:\n",
    "    print(\"Data array:\")\n",
    "    print(\"shape of arr: \", arr_data.shape) \n",
    "    print(\"shape of loaded_array: \", loaded_arr_data.shape)\n",
    "    \n",
    "    # check if both arrays are same or not: \n",
    "    if (arr_data == loaded_arr_data).all(): \n",
    "        print(\"Yes, both the arrays are same\") \n",
    "    else: \n",
    "        print(\"No, both the arrays are not same\")\n",
    "    # check the shapes:\n",
    "    print(\"Label array:\")\n",
    "    print(\"shape of arr: \", arr_label.shape) \n",
    "    print(\"shape of loaded_array: \", loaded_arr_label.shape)\n",
    "\n",
    "    # check if both arrays are same or not: \n",
    "    if (arr_label == loaded_arr_label).all(): \n",
    "        print(\"Yes, both the arrays are same\") \n",
    "    else: \n",
    "        print(\"No, both the arrays are not same\")\n",
    "    return None\n",
    "def load_reshaped_array(timesteps, target_hour, folder_path, data_type=\"train\"):\n",
    "    features = np.load(folder_path + \"/features.npy\", allow_pickle=True).ravel()[0]\n",
    "    loaded_file = np.load(folder_path + \"/{}_{}_{}_data.npz\".format(timesteps, target_hour, data_type), allow_pickle=True)\n",
    "    loaded_data = loaded_file['arr_0']\n",
    "    loaded_data = loaded_data.reshape( \n",
    "            loaded_data.shape[0], loaded_data.shape[1] // features, features).astype(float)\n",
    "    loaded_file.close()\n",
    "    loaded_file_label = np.load(folder_path + \"/{}_{}_{}_label.npz\".format(timesteps, target_hour, data_type), allow_pickle=True)\n",
    "    loaded_label = loaded_file_label['arr_0'].ravel().astype(float)\n",
    "    loaded_file_label.close()\n",
    "    return loaded_data, loaded_label\n",
    "def create_tensorflow_dataset(arr_data, arr_label, batch_size):\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((arr_data, arr_label))\n",
    "    tf_dataset = tf_dataset.repeat().batch(batch_size, drop_remainder=True)\n",
    "    steps_per_epochs = len(arr_data) // batch_size\n",
    "    return tf_dataset, steps_per_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:  (35968, 1, 33)\n",
      "Label shape:  (35968,)\n",
      "Feature shape:  (4438, 1, 33)\n",
      "Label shape:  (4438,)\n",
      "Feature shape:  (3993, 1, 33)\n",
      "Label shape:  (3993,)\n",
      "Data array:\n",
      "shape of arr:  (35968, 1, 33)\n",
      "shape of loaded_array:  (35968, 1, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35968,)\n",
      "shape of loaded_array:  (35968,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3993, 1, 33)\n",
      "shape of loaded_array:  (3993, 1, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3993,)\n",
      "shape of loaded_array:  (3993,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4438, 1, 33)\n",
      "shape of loaded_array:  (4438, 1, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4438,)\n",
      "shape of loaded_array:  (4438,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35966, 2, 33)\n",
      "Label shape:  (35966,)\n",
      "Feature shape:  (4436, 2, 33)\n",
      "Label shape:  (4436,)\n",
      "Feature shape:  (3991, 2, 33)\n",
      "Label shape:  (3991,)\n",
      "Data array:\n",
      "shape of arr:  (35966, 2, 33)\n",
      "shape of loaded_array:  (35966, 2, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35966,)\n",
      "shape of loaded_array:  (35966,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3991, 2, 33)\n",
      "shape of loaded_array:  (3991, 2, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3991,)\n",
      "shape of loaded_array:  (3991,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4436, 2, 33)\n",
      "shape of loaded_array:  (4436, 2, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4436,)\n",
      "shape of loaded_array:  (4436,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35964, 3, 33)\n",
      "Label shape:  (35964,)\n",
      "Feature shape:  (4434, 3, 33)\n",
      "Label shape:  (4434,)\n",
      "Feature shape:  (3989, 3, 33)\n",
      "Label shape:  (3989,)\n",
      "Data array:\n",
      "shape of arr:  (35964, 3, 33)\n",
      "shape of loaded_array:  (35964, 3, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35964,)\n",
      "shape of loaded_array:  (35964,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3989, 3, 33)\n",
      "shape of loaded_array:  (3989, 3, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3989,)\n",
      "shape of loaded_array:  (3989,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4434, 3, 33)\n",
      "shape of loaded_array:  (4434, 3, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4434,)\n",
      "shape of loaded_array:  (4434,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35962, 4, 33)\n",
      "Label shape:  (35962,)\n",
      "Feature shape:  (4432, 4, 33)\n",
      "Label shape:  (4432,)\n",
      "Feature shape:  (3987, 4, 33)\n",
      "Label shape:  (3987,)\n",
      "Data array:\n",
      "shape of arr:  (35962, 4, 33)\n",
      "shape of loaded_array:  (35962, 4, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35962,)\n",
      "shape of loaded_array:  (35962,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3987, 4, 33)\n",
      "shape of loaded_array:  (3987, 4, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3987,)\n",
      "shape of loaded_array:  (3987,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4432, 4, 33)\n",
      "shape of loaded_array:  (4432, 4, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4432,)\n",
      "shape of loaded_array:  (4432,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35960, 5, 33)\n",
      "Label shape:  (35960,)\n",
      "Feature shape:  (4430, 5, 33)\n",
      "Label shape:  (4430,)\n",
      "Feature shape:  (3985, 5, 33)\n",
      "Label shape:  (3985,)\n",
      "Data array:\n",
      "shape of arr:  (35960, 5, 33)\n",
      "shape of loaded_array:  (35960, 5, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35960,)\n",
      "shape of loaded_array:  (35960,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3985, 5, 33)\n",
      "shape of loaded_array:  (3985, 5, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3985,)\n",
      "shape of loaded_array:  (3985,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4430, 5, 33)\n",
      "shape of loaded_array:  (4430, 5, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4430,)\n",
      "shape of loaded_array:  (4430,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35958, 6, 33)\n",
      "Label shape:  (35958,)\n",
      "Feature shape:  (4428, 6, 33)\n",
      "Label shape:  (4428,)\n",
      "Feature shape:  (3983, 6, 33)\n",
      "Label shape:  (3983,)\n",
      "Data array:\n",
      "shape of arr:  (35958, 6, 33)\n",
      "shape of loaded_array:  (35958, 6, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35958,)\n",
      "shape of loaded_array:  (35958,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3983, 6, 33)\n",
      "shape of loaded_array:  (3983, 6, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3983,)\n",
      "shape of loaded_array:  (3983,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4428, 6, 33)\n",
      "shape of loaded_array:  (4428, 6, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4428,)\n",
      "shape of loaded_array:  (4428,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35956, 7, 33)\n",
      "Label shape:  (35956,)\n",
      "Feature shape:  (4426, 7, 33)\n",
      "Label shape:  (4426,)\n",
      "Feature shape:  (3981, 7, 33)\n",
      "Label shape:  (3981,)\n",
      "Data array:\n",
      "shape of arr:  (35956, 7, 33)\n",
      "shape of loaded_array:  (35956, 7, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35956,)\n",
      "shape of loaded_array:  (35956,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3981, 7, 33)\n",
      "shape of loaded_array:  (3981, 7, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3981,)\n",
      "shape of loaded_array:  (3981,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4426, 7, 33)\n",
      "shape of loaded_array:  (4426, 7, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4426,)\n",
      "shape of loaded_array:  (4426,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35954, 8, 33)\n",
      "Label shape:  (35954,)\n",
      "Feature shape:  (4424, 8, 33)\n",
      "Label shape:  (4424,)\n",
      "Feature shape:  (3979, 8, 33)\n",
      "Label shape:  (3979,)\n",
      "Data array:\n",
      "shape of arr:  (35954, 8, 33)\n",
      "shape of loaded_array:  (35954, 8, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35954,)\n",
      "shape of loaded_array:  (35954,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3979, 8, 33)\n",
      "shape of loaded_array:  (3979, 8, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3979,)\n",
      "shape of loaded_array:  (3979,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4424, 8, 33)\n",
      "shape of loaded_array:  (4424, 8, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4424,)\n",
      "shape of loaded_array:  (4424,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35952, 9, 33)\n",
      "Label shape:  (35952,)\n",
      "Feature shape:  (4422, 9, 33)\n",
      "Label shape:  (4422,)\n",
      "Feature shape:  (3977, 9, 33)\n",
      "Label shape:  (3977,)\n",
      "Data array:\n",
      "shape of arr:  (35952, 9, 33)\n",
      "shape of loaded_array:  (35952, 9, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35952,)\n",
      "shape of loaded_array:  (35952,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3977, 9, 33)\n",
      "shape of loaded_array:  (3977, 9, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3977,)\n",
      "shape of loaded_array:  (3977,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4422, 9, 33)\n",
      "shape of loaded_array:  (4422, 9, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4422,)\n",
      "shape of loaded_array:  (4422,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35950, 10, 33)\n",
      "Label shape:  (35950,)\n",
      "Feature shape:  (4420, 10, 33)\n",
      "Label shape:  (4420,)\n",
      "Feature shape:  (3975, 10, 33)\n",
      "Label shape:  (3975,)\n",
      "Data array:\n",
      "shape of arr:  (35950, 10, 33)\n",
      "shape of loaded_array:  (35950, 10, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35950,)\n",
      "shape of loaded_array:  (35950,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3975, 10, 33)\n",
      "shape of loaded_array:  (3975, 10, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3975,)\n",
      "shape of loaded_array:  (3975,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4420, 10, 33)\n",
      "shape of loaded_array:  (4420, 10, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4420,)\n",
      "shape of loaded_array:  (4420,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35948, 11, 33)\n",
      "Label shape:  (35948,)\n",
      "Feature shape:  (4418, 11, 33)\n",
      "Label shape:  (4418,)\n",
      "Feature shape:  (3973, 11, 33)\n",
      "Label shape:  (3973,)\n",
      "Data array:\n",
      "shape of arr:  (35948, 11, 33)\n",
      "shape of loaded_array:  (35948, 11, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35948,)\n",
      "shape of loaded_array:  (35948,)\n",
      "Yes, both the arrays are same\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data array:\n",
      "shape of arr:  (3973, 11, 33)\n",
      "shape of loaded_array:  (3973, 11, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3973,)\n",
      "shape of loaded_array:  (3973,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4418, 11, 33)\n",
      "shape of loaded_array:  (4418, 11, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4418,)\n",
      "shape of loaded_array:  (4418,)\n",
      "Yes, both the arrays are same\n",
      "Feature shape:  (35946, 12, 33)\n",
      "Label shape:  (35946,)\n",
      "Feature shape:  (4416, 12, 33)\n",
      "Label shape:  (4416,)\n",
      "Feature shape:  (3971, 12, 33)\n",
      "Label shape:  (3971,)\n",
      "Data array:\n",
      "shape of arr:  (35946, 12, 33)\n",
      "shape of loaded_array:  (35946, 12, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (35946,)\n",
      "shape of loaded_array:  (35946,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (3971, 12, 33)\n",
      "shape of loaded_array:  (3971, 12, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (3971,)\n",
      "shape of loaded_array:  (3971,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (4416, 12, 33)\n",
      "shape of loaded_array:  (4416, 12, 33)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (4416,)\n",
      "shape of loaded_array:  (4416,)\n",
      "Yes, both the arrays are same\n"
     ]
    }
   ],
   "source": [
    "_data_to_model_path = root_path + 'Data/thudohanoi/data_to_model_hcm'\n",
    "for timesteps in range(1, 13):\n",
    "    for target_hour in [1]:\n",
    "        # Create train, dev, test data\n",
    "        final_df = add_features(thudohanoi_df).copy()\n",
    "        train_df, test_df = generate_train_test_set_by_time(final_df)\n",
    "        train_df, dev_df = generate_train_test_set_by_time(train_df)\n",
    "        train, y_train, multiclass_y = data_preprocessing(train_df, target_hour, timesteps=timesteps)\n",
    "        test, y_test, multiclass_y_test = data_preprocessing(test_df, target_hour, timesteps=timesteps)\n",
    "        dev, y_dev, multiclass_y_dev = data_preprocessing(dev_df, target_hour, timesteps=timesteps)\n",
    "\n",
    "        # Save data to file\n",
    "        reshape_array_and_save_to_path(train, y_train, path=_data_to_model_path, timesteps=timesteps, target_hour=target_hour, data_type=\"train\")\n",
    "        reshape_array_and_save_to_path(dev, y_dev, path=_data_to_model_path, timesteps=timesteps, target_hour=target_hour, data_type=\"dev\")\n",
    "        reshape_array_and_save_to_path(test, y_test, path=_data_to_model_path, timesteps=timesteps, target_hour=target_hour, data_type=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.84966599999998,
   "position": {
    "height": "144.85px",
    "left": "1296.4px",
    "right": "20px",
    "top": "31px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
