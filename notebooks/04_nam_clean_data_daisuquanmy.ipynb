{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PM25</th>\n",
       "      <th>AQI_h</th>\n",
       "      <th>AQI_h_Polutant</th>\n",
       "      <th>AQI_h_label</th>\n",
       "      <th>NowCast</th>\n",
       "      <th>AQI_h_I</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_id</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">49</th>\n",
       "      <th>2016-02-05 14:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-05 15:00:00</th>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-05 16:00:00</th>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>37.448980</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-05 17:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>39.080882</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-05 18:00:00</th>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>39.913255</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-28 20:00:00</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Good</td>\n",
       "      <td>10.572388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-28 21:00:00</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Good</td>\n",
       "      <td>11.785400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-28 22:00:00</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Good</td>\n",
       "      <td>12.391846</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-28 23:00:00</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Good</td>\n",
       "      <td>12.195068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01 00:00:00</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>PM25</td>\n",
       "      <td>Good</td>\n",
       "      <td>11.596436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44411 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             PM25  AQI_h AQI_h_Polutant AQI_h_label  \\\n",
       "site_id time                                                          \n",
       "49      2016-02-05 14:00:00     0      0           PM25        Good   \n",
       "        2016-02-05 15:00:00    68     68           PM25    Moderate   \n",
       "        2016-02-05 16:00:00    75     75           PM25    Moderate   \n",
       "        2016-02-05 17:00:00    78     78           PM25    Moderate   \n",
       "        2016-02-05 18:00:00    80     80           PM25    Moderate   \n",
       "...                           ...    ...            ...         ...   \n",
       "        2021-02-28 20:00:00    21     21           PM25        Good   \n",
       "        2021-02-28 21:00:00    24     24           PM25        Good   \n",
       "        2021-02-28 22:00:00    25     25           PM25        Good   \n",
       "        2021-02-28 23:00:00    24     24           PM25        Good   \n",
       "        2021-03-01 00:00:00    23     23           PM25        Good   \n",
       "\n",
       "                               NowCast  AQI_h_I  \n",
       "site_id time                                     \n",
       "49      2016-02-05 14:00:00   0.000000        1  \n",
       "        2016-02-05 15:00:00  33.750000        2  \n",
       "        2016-02-05 16:00:00  37.448980        2  \n",
       "        2016-02-05 17:00:00  39.080882        2  \n",
       "        2016-02-05 18:00:00  39.913255        2  \n",
       "...                                ...      ...  \n",
       "        2021-02-28 20:00:00  10.572388        1  \n",
       "        2021-02-28 21:00:00  11.785400        1  \n",
       "        2021-02-28 22:00:00  12.391846        1  \n",
       "        2021-02-28 23:00:00  12.195068        1  \n",
       "        2021-03-01 00:00:00  11.596436        1  \n",
       "\n",
       "[44411 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "print(os.path.abspath(PROJ_ROOT))\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from features import calculate_AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to know the number of continuous values\n",
    "# So when we use chunker, we can refer to the starting point to know how many to skip if timeline is not continous after skipping\n",
    "# Basic idea: Loop through the dataframe, if it is continuous increase the continous counter and pos counter\n",
    "# If encounter uncontinous timeline (Encounter a nan value), find the lastpoint with a loop,\n",
    "# Then change the start position continous length column values to the continous counter.\n",
    "# Reset the continous counter and start counter\n",
    "# Result: at the start of each continous timeline, there will be a number which indicate how many step you need to skip if\n",
    "# you need to get to the next timeline\n",
    "\n",
    "def time_continous_marker(df):\n",
    "    df_copy = df.copy()\n",
    "    pos = 0\n",
    "    start_pos = 0\n",
    "    continous = 0\n",
    "    size = 24\n",
    "    length = df_copy.shape[0]\n",
    "    while pos < length:\n",
    "        try:\n",
    "            if df_copy.iloc[pos : pos + size, 1].isnull().sum() == 0:\n",
    "                while df_copy.iloc[pos : pos + size, 1].isnull().sum() == 0:\n",
    "                    pos += size\n",
    "                    continous += size\n",
    "            else:\n",
    "                while np.isnan(df_copy.iloc[pos, 1]) == False:\n",
    "                    continous += 1\n",
    "                    pos += 1\n",
    "                df_copy.iloc[start_pos, 7] = continous\n",
    "                continous = 0\n",
    "                while np.isnan(df_copy.iloc[pos, 1]) == True:\n",
    "                    pos += 1\n",
    "                start_pos = pos\n",
    "#             print(pos)\n",
    "        except IndexError:\n",
    "            df_copy.iloc[start_pos, 7] = continous\n",
    "            continous = 0\n",
    "            print(\"Current position is: {}\".format(pos))\n",
    "            pos = length\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP HCM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "_HCM_us_embass_raw_data_path = os.path.join(PROJ_ROOT,\n",
    "                             \"data\",\n",
    "                             \"external\",\n",
    "                             \"us_embass\",\n",
    "                             \"raw\")\n",
    "\n",
    "_HCM_us_embass_data_files = glob.glob(_HCM_us_embass_raw_data_path + '/HoChiMinhCity_*YTD*.csv')\n",
    "USEm_HCM_df = pd.DataFrame()\n",
    "for file in _HCM_daisuquanmy_data_files:\n",
    "    USEm_HCM_df = USEm_HCM_df.append(pd.read_csv(file, parse_dates=True))\n",
    "\n",
    "# Remove site year 2015 because inconsistency data\n",
    "# daisuquanmy_df = daisuquanmy_df[(daisuquanmy_df['Year'] != 2015)]\n",
    "USEm_HCM_df.drop(columns=['Site', 'Parameter', 'AQI', 'AQI Category','NowCast Conc.', 'Conc. Unit', 'Duration'], inplace=True)\n",
    "USEm_HCM_df['Date (LT)'] = pd.to_datetime(USEm_HCM_df['Date (LT)'])\n",
    "USEm_HCM_df = USEm_HCM_df.rename(columns={'Date (LT)': 'time'})\n",
    "USEm_HCM_df['site_id'] = 49\n",
    "# Remove duplicate, fill in missing index and set ['Date (LT)'] as index and sort\n",
    "USEm_HCM_df.drop_duplicates(subset =\"time\", \n",
    "                     keep = \"first\", inplace = True)\n",
    "USEm_HCM_df = USEm_HCM_df.sort_values(by=['time'])\n",
    "USEm_HCM_df = USEm_HCM_df.set_index('time').asfreq('H').sort_index()\n",
    "USEm_HCM_df.reset_index(drop=False, inplace=True)\n",
    "USEm_HCM_df = USEm_HCM_df.set_index(['site_id', 'time'])\n",
    "# For all Raw Conc <= 0 QC Name must be Missing. Raw Conc change to -999\n",
    "USEm_HCM_df.loc[USEm_HCM_df['Raw Conc.'] <= 0, 'Raw Conc.'] = -999\n",
    "USEm_HCM_df.loc[USEm_HCM_df['Raw Conc.'] >= 600, 'Raw Conc.'] = -999\n",
    "USEm_HCM_df.loc[USEm_HCM_df['Raw Conc.'] <= 0, 'QC Name'] = 'Missing'\n",
    "\n",
    "USEm_HCM_df = USEm_HCM_df.replace(-999, np.nan)\n",
    "# Change Raw Conc to PM25\n",
    "USEm_HCM_df.rename(columns={'Raw Conc.': 'PM25'}, inplace=True)\n",
    "# Fill missing data\n",
    "USEm_HCM_df.fillna(method='ffill', limit=2, inplace=True)\n",
    "USEm_HCM_df.fillna(method='bfill', limit=2, inplace=True)\n",
    "\n",
    "# For year 2015 only, take data after 2015-12-09 14:00:00\n",
    "USEm_HCM_df = USEm_HCM_df.loc[USEm_HCM_df.index.get_level_values(1) >= pd.to_datetime(\"2016-02-05 02:00 PM\")]\n",
    "USEm_HCM_df = USEm_HCM_df[['PM25']]\n",
    "\n",
    "USEm_HCM_df['PM25'] = USEm_HCM_df['PM25'].interpolate()\n",
    "\n",
    "USEm_HCM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b1c3970015b7>:4: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  AQI = AQI.append(pd.Series(),ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position is: 44411\n"
     ]
    }
   ],
   "source": [
    "AQI = calculate_AQI.calculate_AQI_h(USEm_HCM_df)\n",
    "AQI = AQI.replace(0, np.nan).copy()\n",
    "AQI = AQI[1:]\n",
    "AQI['Continous length'] = 0\n",
    "AQI = AQI.append(pd.Series(),ignore_index=True)\n",
    "AQI = time_continous_marker(AQI)\n",
    "AQI = AQI.dropna(subset=['AQI_h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_HCM_us_embass_processed_data_path = os.path.join(PROJ_ROOT,\n",
    "                             \"data\",\n",
    "                             \"external\",\n",
    "                             \"us_embass\",\n",
    "                             \"processed\")\n",
    "AQI.to_csv(_HCM_us_embass_processed_data_path + \"/processed_hcm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now chunker need to be able to output chunks of train data and target in correct order\n",
    "# Basic ideas: loop through each chunk of data (size is size with ,\n",
    "# if the timeline is continous, add data to chunk_data [pos: pos+size]\n",
    "# add data to target [pos+size+target_hour]\n",
    "# if the timeline is not, skip pos according to size\n",
    "# Repeat below till timeline is continous\n",
    "# if after skipping, the timeline is still not continous, skip pos according to continous length colunmns\n",
    "from IPython.core.debugger import set_trace\n",
    "def chunker(seq, size, target_hour, debug = True):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        + Dataframe with PM25, AQI_h, AQI_h_I, Continous length and some other columns\n",
    "            - If data doesn't have Continous length columns, add 1 to data\n",
    "            - Take only PM25, date columns as train\n",
    "            - Take only AQI_h, AQI_h_I as target\n",
    "        + size: length of each chunk\n",
    "        + target_hour: labels for the hour which will be predicted\n",
    "    Ouput: 2 lists: chunk_data, target\n",
    "        + chunk_data: original data chunked to a specific timestep, have shape [..., timeframe, features]\n",
    "        + target: Label for each chunked train/test data, have shape [..., 1]\n",
    "    \"\"\"\n",
    "    if not 'Continous length' in seq.columns:\n",
    "        seq['Continous length'] = 0\n",
    "\n",
    "    timerange = pd.Timedelta(hours=size + target_hour)\n",
    "    chunk_data = []\n",
    "    target = []\n",
    "    multiclass_target = []\n",
    "    pos = 0\n",
    "    length = seq.shape[0]\n",
    "    while pos < (length - size):\n",
    "        try:\n",
    "            if seq.iloc[pos + size + target_hour].name[1] - seq.iloc[pos].name[1] == timerange:\n",
    "                while seq.iloc[pos + size + target_hour].name[1] - seq.iloc[pos].name[1] == timerange:\n",
    "                    chunk_data.append(seq.iloc[pos:pos + size].loc['PM25','Hour','Day of Week'])\n",
    "                    target.append(seq.iloc[pos + size + target_hour].loc['AQI_h'])\n",
    "                    multiclass_target.append(seq.iloc[pos + size + target_hour].loc['AQI_h_I'])\n",
    "                    pos += 1\n",
    "                    if pos + size + target_hour >= length - size:\n",
    "                        print(\"Returned\")\n",
    "                        return np.array(chunk_data), np.array(target), np.array(multiclass_target)\n",
    "            else:\n",
    "                tmp_pos = pos + size + target_hour\n",
    "                while seq.iloc[tmp_pos + size + target_hour].name[1] - seq.iloc[tmp_pos].name[1] != timerange:\n",
    "                    tmp_pos += int(seq.iloc[tmp_pos, 6])\n",
    "                print(\"Jump from {} to {}\".format(pos, tmp_pos))\n",
    "                pos = tmp_pos\n",
    "        except IndexError:\n",
    "            print(\"Current position is: {}\".format(pos))\n",
    "            print(\"Current tmp position is: {}\".format(tmp_pos))\n",
    "            pos = length - size\n",
    "    print(\"Returned\")\n",
    "    return np.array(chunk_data), np.array(target), np.array(multiclass_target)\n",
    "chunk_data, target, multiclass_target = chunker(AQI_copy, 12, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.517px",
    "left": "1154px",
    "right": "20px",
    "top": "314px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}