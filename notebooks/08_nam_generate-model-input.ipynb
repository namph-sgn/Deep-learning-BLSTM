{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "State notebook purpose here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get source folder and append to sys directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/runnable_program\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "print(os.path.abspath(PROJ_ROOT))\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)\n",
    "# Data path example\n",
    "#pump_data_path = os.path.join(PROJ_ROOT,\n",
    "#                              \"data\",\n",
    "#                              \"raw\",\n",
    "#                              \"pumps_train_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/4ba37af6-51fd-47bc-8321-8c500c229114/study/School/KHOA LUAN TOT NGHIEP/runnable_program/notebooks',\n",
       " '/home/nam/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/pythonFiles',\n",
       " '/home/nam/Development/anaconda3/lib/python3.8/site-packages/_pdbpp_path_hack',\n",
       " '/home/nam/program/spark-3.0.0-preview-bin-hadoop2.7/python',\n",
       " '/home/nam/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/pythonFiles',\n",
       " '/home/nam/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/pythonFiles/lib/python',\n",
       " '/home/nam/Development/anaconda3/lib/python38.zip',\n",
       " '/home/nam/Development/anaconda3/lib/python3.8',\n",
       " '/home/nam/Development/anaconda3/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/nam/.local/lib/python3.8/site-packages',\n",
       " '/home/nam/Development/anaconda3/lib/python3.8/site-packages',\n",
       " '/home/nam/Development/anaconda3/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/nam/.ipython',\n",
       " '../src',\n",
       " '../../src']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Import libraries and write settings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import glob\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import holidays\n",
    "\n",
    "# Extract Data\n",
    "%aimport features.extract_features\n",
    "from features import extract_features\n",
    "%aimport data.create_load_transform_processed_data\n",
    "from data import create_load_transform_processed_data\n",
    "# Make dataset\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 1\n",
    "# Use %aimport module to reload each module\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis/Modeling\n",
    "Do work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get interim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing file \n",
      "../data/interim/30.csv\n",
      "Currently processing file \n",
      "../data/interim/9.csv\n",
      "Currently processing file \n",
      "../data/interim/32.csv\n",
      "Currently processing file \n",
      "../data/interim/11.csv\n",
      "Currently processing file \n",
      "../data/interim/40.csv\n",
      "Currently processing file \n",
      "../data/interim/28.csv\n",
      "Currently processing file \n",
      "../data/interim/49.csv\n",
      "Currently processing file \n",
      "../data/interim/46.csv\n",
      "Currently processing file \n",
      "../data/interim/10.csv\n",
      "Currently processing file \n",
      "../data/interim/8.csv\n",
      "Currently processing file \n",
      "../data/interim/25.csv\n",
      "Currently processing file \n",
      "../data/interim/7.csv\n",
      "Currently processing file \n",
      "../data/interim/16.csv\n",
      "Currently processing file \n",
      "../data/interim/42.csv\n",
      "Currently processing file \n",
      "../data/interim/44.csv\n",
      "Currently processing file \n",
      "../data/interim/37.csv\n",
      "Currently processing file \n",
      "../data/interim/1.csv\n",
      "Currently processing file \n",
      "../data/interim/13.csv\n",
      "Currently processing file \n",
      "../data/interim/31.csv\n",
      "Currently processing file \n",
      "../data/interim/26.csv\n",
      "Currently processing file \n",
      "../data/interim/12.csv\n",
      "Currently processing file \n",
      "../data/interim/15.csv\n",
      "Currently processing file \n",
      "../data/interim/39.csv\n",
      "Currently processing file \n",
      "../data/interim/14.csv\n",
      "Currently processing file \n",
      "../data/interim/47.csv\n",
      "Currently processing file \n",
      "../data/interim/48.csv\n",
      "Currently processing file \n",
      "../data/interim/33.csv\n",
      "Currently processing file \n",
      "../data/interim/27.csv\n",
      "Currently processing file \n",
      "../data/interim/41.csv\n",
      "Currently processing file \n",
      "../data/interim/24.csv\n",
      "Currently processing file \n",
      "../data/interim/35.csv\n",
      "Currently processing file \n",
      "../data/interim/43.csv\n",
      "Currently processing file \n",
      "../data/interim/45.csv\n",
      "Currently processing file \n",
      "../data/interim/34.csv\n",
      "Currently processing file \n",
      "../data/interim/29.csv\n",
      "Currently processing file \n",
      "../data/interim/38.csv\n",
      "Currently processing file \n",
      "../data/interim/36.csv\n"
     ]
    }
   ],
   "source": [
    "_interim_data_path = os.path.join(PROJ_ROOT,\n",
    "                                  \"data\",\n",
    "                                  \"interim\")\n",
    "_interim_files = glob.glob(_interim_data_path + '/*.csv')\n",
    "\n",
    "interim_df = pd.DataFrame()\n",
    "for file in _interim_files:\n",
    "    print('Currently processing file \\n{}'.format(file))\n",
    "    interim_df = interim_df.append(pd.read_csv(file, parse_dates=True, index_col=['site_id', 'time'],\n",
    "                                                     dtype={'CO': np.float64, 'NO2': np.float64, 'PM25': np.float64,\n",
    "                                                            'AQI_h': np.float64, 'AQI_h_I': np.int, 'site_id': np.int}))\n",
    "# Site 16 have many inconsistency in data so we remove it\n",
    "interim_df = interim_df[(interim_df.index.get_level_values(0) != 16)]\n",
    "# Get only columns we need\n",
    "interim_df = interim_df[['PM25', 'AQI_h', 'AQI_h_Polutant', 'AQI_h_I',\n",
    "       'AQI_h_label', 'Continous length']]\n",
    "# Ho Chi Minh data is on site 49\n",
    "hanoi_df = interim_df[(interim_df.index.get_level_values(0) != 49)].copy()\n",
    "hcm_df = interim_df[(interim_df.index.get_level_values(0) == 49)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input for model from interim data and put in to processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:  (127813, 1, 68)\n",
      "Label shape:  (127813,)\n",
      "Feature shape:  (16363, 1, 68)\n",
      "Label shape:  (16363,)\n",
      "Feature shape:  (14744, 1, 68)\n",
      "Label shape:  (14744,)\n",
      "Data array:\n",
      "shape of arr:  (127813, 1, 68)\n",
      "shape of loaded_array:  (127813, 1, 68)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (127813,)\n",
      "shape of loaded_array:  (127813,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (14744, 1, 68)\n",
      "shape of loaded_array:  (14744, 1, 68)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (14744,)\n",
      "shape of loaded_array:  (14744,)\n",
      "Yes, both the arrays are same\n",
      "Data array:\n",
      "shape of arr:  (16363, 1, 68)\n",
      "shape of loaded_array:  (16363, 1, 68)\n",
      "Yes, both the arrays are same\n",
      "Label array:\n",
      "shape of arr:  (16363,)\n",
      "shape of loaded_array:  (16363,)\n",
      "Yes, both the arrays are same\n",
      "Input have been created\n"
     ]
    }
   ],
   "source": [
    "_processed_data_path = os.path.join(PROJ_ROOT,\n",
    "                                   \"data\",\n",
    "                                   \"processed\")\n",
    "\n",
    "def create_input_for_model(df, timesteps=[1], target_hour=[1], output_path=None):\n",
    "    \"\"\"From interim dataframe:\n",
    "        + add features\n",
    "        + split into chunks according to timesteps\n",
    "        + compressed and saved to output_path\n",
    "        + estimate number of created dataset = timesteps * target_hour\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Contains interim data.\n",
    "    timesteps : list of integer\n",
    "        Each timestep represent 1 dataset\n",
    "    target_hour : list of integer\n",
    "        the label for each timesteps\n",
    "    output_path : string\n",
    "        Destination directory the dataset will be created\n",
    "    \"\"\"\n",
    "    if output_path == None:\n",
    "        output_path == os.path.join(PROJ_ROOT,\n",
    "                                   \"data\",\n",
    "                                   \"processed\")\n",
    "    for timesteps in timesteps:\n",
    "        for target_hour in target_hour:\n",
    "            # Create train, dev, test data\n",
    "            final_df = extract_features.add_features(df).copy()\n",
    "            train_df, test_df = extract_features.generate_train_test_set_by_time(final_df)\n",
    "            train_df, dev_df = extract_features.generate_train_test_set_by_time(train_df)\n",
    "            train, y_train, multiclass_y = extract_features.data_preprocessing(train_df, target_hour, timesteps=timesteps)\n",
    "            test, y_test, multiclass_y_test = extract_features.data_preprocessing(test_df, target_hour, timesteps=timesteps)\n",
    "            dev, y_dev, multiclass_y_dev = extract_features.data_preprocessing(dev_df, target_hour, timesteps=timesteps)\n",
    "\n",
    "            # Save data to file\n",
    "            create_model_input.reshape_array_and_save_to_path(train, y_train, path=output_path, timesteps=timesteps, target_hour=target_hour, data_type=\"train\")\n",
    "            create_model_input.reshape_array_and_save_to_path(dev, y_dev, path=output_path, timesteps=timesteps, target_hour=target_hour, data_type=\"dev\")\n",
    "            create_model_input.reshape_array_and_save_to_path(test, y_test, path=output_path, timesteps=timesteps, target_hour=target_hour, data_type=\"test\")\n",
    "    print(\"Input have been created\")\n",
    "create_input_for_model(hanoi_df, output_path=_processed_data_path+\"/hanoi\")\n",
    "create_input_for_model(hcm_df, output_path=_processed_data_path+\"/hcm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Show graphs and stats here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next Steps\n",
    "Summarize findings here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}